{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4ef861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code uses a CPU...\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "atom_dict = defaultdict(lambda: len(atom_dict))\n",
    "bond_dict = defaultdict(lambda: len(bond_dict))\n",
    "fingerprint_dict = defaultdict(lambda: len(fingerprint_dict))\n",
    "edge_dict = defaultdict(lambda: len(edge_dict))\n",
    "radius=1\n",
    "\n",
    "def dump_dictionary(dictionary, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(dict(dictionary), f)\n",
    "        \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('The code uses a GPU!')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('The code uses a CPU...')\n",
    "\t\n",
    "def create_atoms(mol, atom_dict):\n",
    "    \"\"\"Transform the atom types in a molecule (e.g., H, C, and O)\n",
    "    into the indices (e.g., H=0, C=1, and O=2).\n",
    "    Note that each atom index considers the aromaticity.\n",
    "    \"\"\"\n",
    "    atoms = [a.GetSymbol() for a in mol.GetAtoms()]\n",
    "    for a in mol.GetAromaticAtoms():\n",
    "        i = a.GetIdx()\n",
    "        atoms[i] = (atoms[i], 'aromatic')\n",
    "    atoms = [atom_dict[a] for a in atoms]\n",
    "    return np.array(atoms)\n",
    "\n",
    "\n",
    "def create_ijbonddict(mol, bond_dict):\n",
    "    \"\"\"Create a dictionary, in which each key is a node ID\n",
    "    and each value is the tuples of its neighboring node\n",
    "    and chemical bond (e.g., single and double) IDs.\n",
    "    \"\"\"\n",
    "    i_jbond_dict = defaultdict(lambda: [])\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bond = bond_dict[str(b.GetBondType())]\n",
    "        i_jbond_dict[i].append((j, bond))\n",
    "        i_jbond_dict[j].append((i, bond))\n",
    "    return i_jbond_dict\n",
    "\n",
    "\n",
    "def extract_fingerprints(radius, atoms, i_jbond_dict,\n",
    "                         fingerprint_dict, edge_dict):\n",
    "    \"\"\"Extract the fingerprints from a molecular graph\n",
    "    based on Weisfeiler-Lehman algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    if (len(atoms) == 1) or (radius == 0):\n",
    "        nodes = [fingerprint_dict[a] for a in atoms]\n",
    "\n",
    "    else:\n",
    "        nodes = atoms\n",
    "        i_jedge_dict = i_jbond_dict\n",
    "\n",
    "        for _ in range(radius):\n",
    "\n",
    "            \"\"\"Update each node ID considering its neighboring nodes and edges.\n",
    "            The updated node IDs are the fingerprint IDs.\n",
    "            \"\"\"\n",
    "            nodes_ = []\n",
    "            for i, j_edge in i_jedge_dict.items():\n",
    "                neighbors = [(nodes[j], edge) for j, edge in j_edge]\n",
    "                fingerprint = (nodes[i], tuple(sorted(neighbors)))\n",
    "                nodes_.append(fingerprint_dict[fingerprint])\n",
    "\n",
    "            \"\"\"Also update each edge ID considering\n",
    "            its two nodes on both sides.\n",
    "            \"\"\"\n",
    "            i_jedge_dict_ = defaultdict(lambda: [])\n",
    "            for i, j_edge in i_jedge_dict.items():\n",
    "                for j, edge in j_edge:\n",
    "                    both_side = tuple(sorted((nodes[i], nodes[j])))\n",
    "                    edge = edge_dict[(both_side, edge)]\n",
    "                    i_jedge_dict_[i].append((j, edge))\n",
    "\n",
    "            nodes = nodes_\n",
    "            i_jedge_dict = i_jedge_dict_\n",
    "\n",
    "    return np.array(nodes)\n",
    "\n",
    "def create_dataset(filename,path,dataname):\n",
    "    dir_dataset = path\n",
    "    print(filename)\n",
    "    \"\"\"Load a dataset.\"\"\"\n",
    "    with open(dir_dataset + filename, 'r') as f:\n",
    "        smiles_property = f.readline().strip().split()\n",
    "        data_original = f.read().strip().split('\\n')\n",
    "\n",
    "        \"\"\"Exclude the data contains '.' in its smiles.\"\"\"\n",
    "    data_original = [data for data in data_original\n",
    "                        if '.' not in data.split()[0]]\n",
    "    dataset = []\n",
    "    for data in data_original:\n",
    "\n",
    "        smiles, property = data.strip().split()\n",
    "\n",
    "        \"\"\"Create each data with the above defined functions.\"\"\"\n",
    "        mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "        atoms = create_atoms(mol, atom_dict)\n",
    "        molecular_size = len(atoms)\n",
    "        i_jbond_dict = create_ijbonddict(mol, bond_dict)\n",
    "        fingerprints = extract_fingerprints(radius, atoms, i_jbond_dict,\n",
    "                                                fingerprint_dict, edge_dict)\n",
    "        adjacency = np.float32((Chem.GetAdjacencyMatrix(mol)))\n",
    "#Transform the above each data of numpy to pytorch tensor on a device (i.e., CPU or GPU).\n",
    "        fingerprints = torch.LongTensor(fingerprints).to(device)\n",
    "        adjacency = torch.FloatTensor(adjacency).to(device)\n",
    "        property = torch.FloatTensor([[float(property)]]).to(device)\n",
    "        dataset.append((smiles,fingerprints, adjacency, molecular_size, property))\n",
    "    dir_dataset=path\n",
    "    dump_dictionary(fingerprint_dict, dir_dataset +dataname+ '-fingerprint_dict.pickle')\n",
    "    dump_dictionary(atom_dict, dir_dataset +dataname+ '-atom_dict.pickle')\n",
    "    dump_dictionary(bond_dict, dir_dataset  +dataname+ '-bond_dict.pickle')\n",
    "    dump_dictionary(edge_dict, dir_dataset +dataname+ '-edge_dict.pickle')\n",
    "    return dataset\n",
    "\t\n",
    "def create_dataset_randomsplit(x,y,path,dataname):\n",
    "    dir_input = path + 'SMRT-'\n",
    "    with open(dir_input + 'atom_dict.pickle', 'rb') as f:\n",
    "        c=pickle.load(f)\n",
    "        for k in c.keys():\n",
    "            atom_dict.get(k)\n",
    "            atom_dict[k]=c[k]\n",
    "    with open(dir_input+ 'bond_dict.pickle', 'rb') as f:\n",
    "        c=pickle.load(f)\n",
    "        for k in c.keys():\n",
    "            bond_dict.get(k)\n",
    "            bond_dict[k]=c[k]\n",
    "        \n",
    "    with open(dir_input + 'edge_dict.pickle', 'rb') as f:\n",
    "        c=pickle.load(f)\n",
    "        for k in c.keys():\n",
    "            edge_dict.get(k)\n",
    "            edge_dict[k]=c[k]\n",
    "        \n",
    "    with open(dir_input + 'fingerprint_dict.pickle', 'rb') as f:\n",
    "        c=pickle.load(f)\n",
    "        for k in c.keys():\n",
    "            fingerprint_dict.get(k)\n",
    "            fingerprint_dict[k]=c[k]    \n",
    "    dataset = []  \n",
    "    for i in range(len(x)):\n",
    "        smiles=x[i]\n",
    "        property=y[i]         \n",
    "        \"\"\"Create each data with the above defined functions.\"\"\"\n",
    "        mol = Chem.MolFromInchi(smiles)     \n",
    "        mol = Chem.AddHs(Chem.MolFromInchi(smiles))\n",
    "        atoms = create_atoms(mol, atom_dict)\n",
    "        molecular_size = len(atoms)\n",
    "        i_jbond_dict = create_ijbonddict(mol, bond_dict)\n",
    "        fingerprints = extract_fingerprints(radius, atoms, i_jbond_dict,\n",
    "                                                fingerprint_dict, edge_dict)\n",
    "        adjacency = np.float32((Chem.GetAdjacencyMatrix(mol)))\n",
    "#Transform the above each data of numpy to pytorch tensor on a device (i.e., CPU or GPU).\n",
    "        fingerprints = torch.LongTensor(fingerprints).to(device)\n",
    "        adjacency = torch.FloatTensor(adjacency).to(device)\n",
    "        property = torch.FloatTensor([[float(property)]]).to(device)\n",
    "\n",
    "        dataset.append((smiles,fingerprints, adjacency, molecular_size, property))\n",
    "    dir_dataset=path\n",
    "    dump_dictionary(fingerprint_dict, dir_dataset +dataname+ '-fingerprint_dict.pickle')\n",
    "    dump_dictionary(atom_dict, dir_dataset +dataname+ '-atom_dict.pickle')\n",
    "    dump_dictionary(bond_dict, dir_dataset  +dataname+ '-bond_dict.pickle')\n",
    "    dump_dictionary(edge_dict, dir_dataset +dataname+ '-edge_dict.pickle')\n",
    "    return dataset\n",
    "\t\n",
    "def create_dataset_kfold(x,y,path,dataname):\n",
    "    dir_input =path+'SMRT-'\n",
    "    with open(dir_input + 'atom_dict.pickle', 'rb') as f:\n",
    "        c=pickle.load(f)\n",
    "        for k in c.keys():\n",
    "            atom_dict.get(k)\n",
    "            atom_dict[k]=c[k]\n",
    "    with open(dir_input+ 'bond_dict.pickle', 'rb') as f:\n",
    "        c=pickle.load(f)\n",
    "        for k in c.keys():\n",
    "            bond_dict.get(k)\n",
    "            bond_dict[k]=c[k]\n",
    "        \n",
    "    with open(dir_input + 'edge_dict.pickle', 'rb') as f:\n",
    "        c=pickle.load(f)\n",
    "        for k in c.keys():\n",
    "            edge_dict.get(k)\n",
    "            edge_dict[k]=c[k]\n",
    "        \n",
    "    with open(dir_input + 'fingerprint_dict.pickle', 'rb') as f:\n",
    "        c=pickle.load(f)\n",
    "        for k in c.keys():\n",
    "            fingerprint_dict.get(k)\n",
    "            fingerprint_dict[k]=c[k]   \n",
    "    dataset = []\n",
    "    for i in range(len(x)):\n",
    "        smiles=x[i]\n",
    "        property=y[i]\n",
    "        \"\"\"Create each data with the above defined functions.\"\"\"\n",
    "        mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "        atoms = create_atoms(mol, atom_dict)\n",
    "        molecular_size = len(atoms)\n",
    "        i_jbond_dict = create_ijbonddict(mol, bond_dict)\n",
    "        fingerprints = extract_fingerprints(radius, atoms, i_jbond_dict,\n",
    "                                                fingerprint_dict, edge_dict)\n",
    "        adjacency = np.float32((Chem.GetAdjacencyMatrix(mol)))\n",
    "#Transform the above each data of numpy to pytorch tensor on a device (i.e., CPU or GPU).\n",
    "        fingerprints = torch.LongTensor(fingerprints).to(device)\n",
    "        adjacency = torch.FloatTensor(adjacency).to(device)\n",
    "        property = torch.FloatTensor([[float(property)]]).to(device)\n",
    "        dataset.append((smiles,fingerprints, adjacency, molecular_size, property))\n",
    "    dir_dataset=path\n",
    "    dump_dictionary(fingerprint_dict, dir_dataset +dataname+ '-fingerprint_dict.pickle')\n",
    "    dump_dictionary(atom_dict, dir_dataset +dataname+ '-atom_dict.pickle')\n",
    "    dump_dictionary(bond_dict, dir_dataset  +dataname+ '-bond_dict.pickle')\n",
    "    dump_dictionary(edge_dict, dir_dataset +dataname+ '-edge_dict.pickle')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def transferlearning_dataset_predict(x,path):\n",
    "    dir_input = path+'SMRT-'\n",
    "    with open(dir_input + 'atom_dict.pickle', 'rb') as f:\n",
    "        c=pickle.load(f)\n",
    "        for k in c.keys():\n",
    "            atom_dict.get(k)\n",
    "            atom_dict[k]=c[k]\n",
    "    with open(dir_input+ 'bond_dict.pickle', 'rb') as f:\n",
    "        c=pickle.load(f)\n",
    "        for k in c.keys():\n",
    "            bond_dict.get(k)\n",
    "            bond_dict[k]=c[k]\n",
    "        \n",
    "    with open(dir_input + 'edge_dict.pickle', 'rb') as f:\n",
    "        c=pickle.load(f)\n",
    "        for k in c.keys():\n",
    "            edge_dict.get(k)\n",
    "            edge_dict[k]=c[k]\n",
    "        \n",
    "    with open(dir_input + 'fingerprint_dict.pickle', 'rb') as f:\n",
    "        c=pickle.load(f)\n",
    "        for k in c.keys():\n",
    "            fingerprint_dict.get(k)\n",
    "            fingerprint_dict[k]=c[k]\n",
    "    dataset = []\n",
    "    for i in range(len(x)):\n",
    "        smiles=x[i]\n",
    "        \"\"\"Create each data with the above defined functions.\"\"\"       \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue           \n",
    "        else:\n",
    "            smi = Chem.MolToSmiles(mol)            \n",
    "        mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "        atoms = create_atoms(mol, atom_dict)\n",
    "        molecular_size = len(atoms)\n",
    "        i_jbond_dict = create_ijbonddict(mol, bond_dict)\n",
    "        fingerprints = extract_fingerprints(radius, atoms, i_jbond_dict,\n",
    "                                                fingerprint_dict, edge_dict)\n",
    "        adjacency = np.float32((Chem.GetAdjacencyMatrix(mol)))\n",
    "#Transform the above each data of numpy to pytorch tensor on a device (i.e., CPU or GPU).\n",
    "        fingerprints = torch.LongTensor(fingerprints).to(device)\n",
    "        adjacency = torch.FloatTensor(adjacency).to(device)\n",
    "        dataset.append((smiles,fingerprints, adjacency, molecular_size)) \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad1d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import timeit\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import preprocess as pp\n",
    "import pickle\n",
    "import random\n",
    "class MolecularGraphNeuralNetwork(nn.Module):\n",
    "    def __init__(self, N, dim, layer_hidden, layer_output):\n",
    "        super(MolecularGraphNeuralNetwork, self).__init__()\n",
    "        self.embed_fingerprint = nn.Embedding(N, dim)\n",
    "        self.W_fingerprint = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                            for _ in range(layer_hidden)])\n",
    "        self.W_output = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                       for _ in range(layer_output)])\n",
    "        self.W_property = nn.Linear(dim, 1)\n",
    "\n",
    "    def pad(self, matrices, pad_value):\n",
    "        \"\"\"Pad the list of matrices\n",
    "        with a pad_value (e.g., 0) for batch processing.\n",
    "        For example, given a list of matrices [A, B, C],\n",
    "        we obtain a new matrix [A00, 0B0, 00C],\n",
    "        where 0 is the zero (i.e., pad value) matrix.\n",
    "        \"\"\"\n",
    "        shapes = [m.shape for m in matrices]\n",
    "        M, N = sum([s[0] for s in shapes]), sum([s[1] for s in shapes])\n",
    "        zeros = torch.FloatTensor(np.zeros((M, N))).to(device)\n",
    "        pad_matrices = pad_value + zeros\n",
    "        i, j = 0, 0\n",
    "        for k, matrix in enumerate(matrices):\n",
    "            m, n = shapes[k]\n",
    "            pad_matrices[i:i+m, j:j+n] = matrix\n",
    "            i += m\n",
    "            j += n\n",
    "        return pad_matrices\n",
    "\n",
    "    def update(self, matrix, vectors, layer):\n",
    "        hidden_vectors = torch.relu(self.W_fingerprint[layer](vectors))\n",
    "        return hidden_vectors + torch.matmul(matrix, hidden_vectors)\n",
    "\n",
    "    def sum(self, vectors, axis):\n",
    "        sum_vectors = [torch.sum(v, 0) for v in torch.split(vectors, axis)]\n",
    "        return torch.stack(sum_vectors)\n",
    "\n",
    "    def mean(self, vectors, axis):\n",
    "        mean_vectors = [torch.mean(v, 0) for v in torch.split(vectors, axis)]\n",
    "        return torch.stack(mean_vectors)\n",
    "\n",
    "    def gnn(self, inputs):\n",
    "\n",
    "        \"\"\"Cat or pad each input data for batch processing.\"\"\"\n",
    "        Smiles,fingerprints, adjacencies, molecular_sizes = inputs\n",
    "        fingerprints = torch.cat(fingerprints)\n",
    "        adjacencies = self.pad(adjacencies, 0)\n",
    "\n",
    "        \"\"\"GNN layer (update the fingerprint vectors).\"\"\"\n",
    "        fingerprint_vectors = self.embed_fingerprint(fingerprints)\n",
    "        for l in range(layer_hidden):\n",
    "            hs = self.update(adjacencies, fingerprint_vectors, l)\n",
    "            fingerprint_vectors = F.normalize(hs, 2, 1)  # normalize.\n",
    "\n",
    "        \"\"\"Molecular vector by sum or mean of the fingerprint vectors.\"\"\"\n",
    "        molecular_vectors = self.sum(fingerprint_vectors, molecular_sizes)\n",
    "        return Smiles,molecular_vectors\n",
    "\n",
    "    def mlp(self, vectors):\n",
    "        \"\"\" regressor based on multilayer perceptron.\"\"\"\n",
    "        for l in range(layer_output):\n",
    "            vectors = torch.relu(self.W_output[l](vectors))\n",
    "        outputs = self.W_property(vectors)\n",
    "        return outputs\n",
    "    def forward_regressor(self, data_batch, train):\n",
    "\n",
    "        inputs = data_batch[:-1]\n",
    "        correct_values = torch.cat(data_batch[-1])\n",
    "\n",
    "        if train:\n",
    "            Smiles,molecular_vectors = self.gnn(inputs)\n",
    "            predicted_values = self.mlp(molecular_vectors)\n",
    "            loss = F.mse_loss(predicted_values, correct_values)\n",
    "            return loss\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Smiles,molecular_vectors = self.gnn(inputs)\n",
    "                predicted_values = self.mlp(molecular_vectors)\n",
    "            predicted_values = predicted_values.to('cpu').data.numpy()\n",
    "            correct_values = correct_values.to('cpu').data.numpy()\n",
    "            predicted_values = np.concatenate(predicted_values)\n",
    "            correct_values = np.concatenate(correct_values)\n",
    "            return Smiles,predicted_values, correct_values\n",
    "    def forward_predict(self, data_batch):\n",
    "\n",
    "            inputs = data_batch\n",
    "            Smiles,molecular_vectors = self.gnn(inputs)\n",
    "            predicted_values = self.mlp(molecular_vectors)\n",
    "            predicted_values = predicted_values.to('cpu').data.numpy()\n",
    "            predicted_values = np.concatenate(predicted_values)\n",
    "            \n",
    "            return Smiles,predicted_values\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, dataset):\n",
    "        np.random.shuffle(dataset)\n",
    "        N = len(dataset)\n",
    "        loss_total = 0\n",
    "        for i in range(0, N, batch_train):\n",
    "            data_batch = list(zip(*dataset[i:i+batch_train]))\n",
    "            loss = self.model.forward_regressor(data_batch, train=True)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.item()\n",
    "        return loss_total\n",
    "class Trainer_tf(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    def train(self, dataset):\n",
    "        np.random.shuffle(dataset)\n",
    "        N = len(dataset)\n",
    "        loss_total = 0\n",
    "        for i in range(0, N, batch_train):\n",
    "            data_batch = list(zip(*dataset[i:i+batch_train]))\n",
    "            loss = self.model.forward_regressor(data_batch, train=True)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.item()\n",
    "        return loss_total\n",
    "\n",
    "\n",
    "class Tester(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def test_regressor(self, dataset):\n",
    "        N = len(dataset)\n",
    "        SMILES, Ts, Ys = '', [], []\n",
    "        SAE = 0  # sum absolute error.\n",
    "        for i in range(0, N, batch_test):\n",
    "            data_batch = list(zip(*dataset[i:i+batch_test]))\n",
    "            (Smiles,  predicted_values,correct_values) = self.model.forward_regressor(\n",
    "                                               data_batch, train=False)\n",
    "            SMILES += ' '.join(Smiles) + ' '\n",
    "            Ts.append(correct_values)\n",
    "            Ys.append(predicted_values)\n",
    "            \n",
    "            SAE += sum(np.abs(predicted_values-correct_values))\n",
    "        SMILES = SMILES.strip().split()\n",
    "        T, Y = map(str, np.concatenate(Ts)), map(str, np.concatenate(Ys))\n",
    "        predictions = '\\n'.join(['\\t'.join(x) for x in zip(SMILES, T, Y)])\n",
    "        MAEs = SAE / N  # mean absolute error.\n",
    "        return MAEs,predictions\n",
    "    def test_predict(self, dataset):\n",
    "        N = len(dataset)\n",
    "        SMILES, Ts, Ys = '', [], []\n",
    "        SAE = 0  # sum absolute error.\n",
    "        for i in range(0, N, batch_test):\n",
    "            data_batch = list(zip(*dataset[i:i+batch_test]))\n",
    "            (Smiles,  predicted_values) = self.model.forward_predict(\n",
    "                                               data_batch)\n",
    "            SMILES += ' '.join(Smiles) + ' '\n",
    "            Ys.append(predicted_values)\n",
    "        SMILES = SMILES.strip().split()\n",
    "        Y = map(str, np.concatenate(Ys))\n",
    "        predictions = '\\n'.join(['\\t'.join(x) for x in zip(SMILES, Y)])\n",
    "        return predictions\n",
    "\n",
    "    def save_MAEs(self, MAEs, filename):\n",
    "        with open(filename, 'a') as f:\n",
    "            f.write(MAEs + '\\n')\n",
    "    def save_predictions(self, predictions, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write('Smiles\\tCorrect\\tPredict\\n')\n",
    "            f.write(predictions + '\\n')\n",
    "    def save_model(self, model, filename):\n",
    "        torch.save(model.state_dict(), filename)\n",
    "\n",
    "def split_dataset(dataset, ratio):\n",
    "#   \"\"\"Shuffle and split a dataset.\"\"\"\n",
    "    np.random.seed(1234)  # fix the seed for shuffle.\n",
    "    np.random.shuffle(dataset)\n",
    "    n = int(ratio * len(dataset))\n",
    "    return dataset[:n], dataset[n:]\n",
    "def dump_dictionary(dictionary, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(dict(dictionary), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f252b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'test.xlsx')\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    radius=1\n",
    "    dim=48\n",
    "    layer_hidden=6\n",
    "    layer_output=6\n",
    "    batch_train=32\n",
    "    batch_test=32\n",
    "    lr=1e-4\n",
    "    lr_decay=0.85\n",
    "    decay_interval=10\n",
    "    iteration=200\n",
    "    N=5000\n",
    "    #Path of the pre-trained model\n",
    "    path = 'E:\\\\RT_prediction_for_user\\\\data\\\\'\n",
    "    dataname='SMRT'\n",
    "    \n",
    "    # Check if GPU is available, otherwise use CPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print('The code uses a GPU!')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print('The code uses a CPU...')\n",
    "   \n",
    "    print('The code uses a CPU!')\n",
    "\n",
    "    print('-'*100)\n",
    "    print('The preprocess has finished!')\n",
    "    print('-'*100)\n",
    "    print('Creating a model.')\n",
    "    \n",
    "    torch.manual_seed(1234)\n",
    "    model = MolecularGraphNeuralNetwork(\n",
    "            N, dim, layer_hidden, layer_output).to(device)\n",
    "    \n",
    "    # Load the pre-trained model\n",
    "    file_model = path + 'SMRT_model' + '.h5'\n",
    "    model.load_state_dict(torch.load(file_model, map_location=torch.device('cpu')))\n",
    "    \n",
    "    # Freeze the parameters of W_fingerprint\n",
    "    for para in model.W_fingerprint.parameters():\n",
    "        para.requires_grad = False   \n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    trainer = Trainer_tf(model)\n",
    "    tester = Tester(model)\n",
    "    \n",
    "    # Load external dataset\n",
    "    data_external = df\n",
    "    x_external = list(data_external['SMILES'])\n",
    "    \n",
    "    x_external_str = [str(smiles) for smiles in x_external]\n",
    "    \n",
    "    # Preprocess the external dataset and create a unique list\n",
    "    x_external_cleaned = []\n",
    "    x_external_str_unique = list(set(x_external_str))\n",
    "    \n",
    "    for smiles in x_external_str_unique:\n",
    "        # Try to construct the molecular object\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "        # Check if the molecular object is valid\n",
    "        if mol is not None and smiles.find('.') == -1:\n",
    "            # If valid, proceed with further processing\n",
    "            x_external_cleaned.append(smiles)\n",
    "    \n",
    "    y_external = [0] * len(x_external_cleaned)\n",
    "\n",
    "    # Preprocess the external dataset\n",
    "    dataset_tf_external = create_dataset_kfold(x_external_cleaned, y_external, path, dataname)\n",
    "    \n",
    "    # Make predictions on the external dataset\n",
    "    predictions_external = tester.test_regressor(dataset_tf_external)[1]\n",
    "    \n",
    "    # Save the predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3542873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "# Convert the string to a StringIO object\n",
    "data_io = StringIO(predictions_external)\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "predictions = pd.read_csv(data_io, sep='\\t', header=None, names=['SMILES', 'Correct', 'Predict_RI'])\n",
    "\n",
    "# Merge the predictions with the original DataFrame on the SMILES column\n",
    "dfnew = df.merge(predictions[['SMILES', 'Predict_RI']], on='SMILES', how='left')\n",
    "\n",
    "# Fentanyl chromatographic retention equation: RT = 0.01239 * RI - 3.27332\n",
    "dfnew['Predict_RT'] = 0.01239 * dfnew['Predict_RI'] - 3.27332\n",
    "\n",
    "# Save the resulting DataFrame to an Excel file\n",
    "dfnew.to_excel('output.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b9f46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-scz",
   "language": "python",
   "name": "py311-scz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
